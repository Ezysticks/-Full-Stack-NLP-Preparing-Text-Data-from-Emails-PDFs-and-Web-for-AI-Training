{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ac184a",
   "metadata": {},
   "source": [
    "## 🚀 Full-Stack NLP: Preparing Text Data from Emails, PDFs, and Web for AI Training\n",
    "\n",
    "Full Data Pipeline\n",
    "* ✅ Extract emails from Gmail (IMAP)\n",
    "* ✅ Extract text from PDF (pdfplumber)\n",
    "* ✅ Clean Extracted Text (PDF & Email)\n",
    "* ✅ Scrape text data from websites (BeautifulSoup)\n",
    "* ✅ Convert text data (Scrape, Email & PDF) to JSONL for LLM fine-tuning\n",
    "    * ✅ Extra: Load JSONL Data into Pandas for Data manipulation & Analysis\n",
    "* ✅ Fine-tune an LLM (Hugging Face Transformers)\n",
    "* ✅ Load the Fine-Tuned Model to Test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c2345",
   "metadata": {},
   "source": [
    "\n",
    "#### 📌 Step 1: Setting Up Your Environment\n",
    "\n",
    "Run to Install the following required dependencies/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca6c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\user\\anaconda3\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: imaplib2 in c:\\users\\user\\anaconda3\\lib\\site-packages (3.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: jsonlines in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: dotenv in c:\\users\\user\\anaconda3\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonlines) (25.1.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\anaconda3\\lib\\site-packages (from dotenv) (1.0.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pdfplumber imaplib2 beautifulsoup4 requests nltk jsonlines dotenv selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da7c19",
   "metadata": {},
   "source": [
    "These dependencies are commonly used for data extraction, web scraping, and natural language processing (NLP). Here's what each package does:  \n",
    "\n",
    "1. **`pandas`** – Used for data manipulation and analysis, especially for handling structured data in DataFrames.  \n",
    "2. **`pdfplumber`** – Extracts text and images from PDF files.  \n",
    "3. **`imaplib2`** – An extended version of Python's `imaplib`, used to interact with email servers via IMAP. \n",
    "4. **`beautifulsoup4`** – Parses and extracts data from HTML and XML documents (often used for web scraping).  \n",
    "5. **`requests`** – Sends HTTP requests to web pages and APIs (commonly used in web scraping and API interactions).  \n",
    "6. **`nltk`** – The Natural Language Toolkit for NLP tasks such as tokenization, stemming, and sentiment analysis.  \n",
    "7. **`jsonlines`** – Reads and writes data in JSON Lines format (`.jsonl`), which is useful for handling large JSON datasets efficiently.  \n",
    "8. **`dotenv`** – Loads environment variables from a `.env` file, useful for storing API keys and secrets securely.  \n",
    "9. **`selenium`** – Automates web browser interactions, often used for web scraping when JavaScript rendering is required.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281fe35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.6.0+cpu)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face Transformers & Datasets\n",
    "\n",
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1e076",
   "metadata": {},
   "source": [
    "These dependencies are essential for working with deep learning models, particularly in **Natural Language Processing (NLP)** and **Large Language Model (LLM) fine-tuning**:\n",
    "\n",
    "1. **`transformers`** – A library by Hugging Face for using and fine-tuning pre-trained transformer models like BERT, GPT, and T5.  \n",
    "2. **`datasets`** – A lightweight library by Hugging Face for loading, processing, and managing large-scale datasets efficiently.  \n",
    "3. **`torch`** – The core PyTorch library, required for building and training deep learning models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9476a8",
   "metadata": {},
   "source": [
    "#### 📌 Step 2: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b05768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract Emails from Gmail\n",
    "'''\n",
    "import os  # To interact with the operating system, including handling environment variables.\n",
    "from dotenv import load_dotenv  # loads environment variables from a .env file into os.environ\n",
    "\n",
    "import imaplib  # Import the imaplib module for connecting to and interacting with an email server using IMAP.\n",
    "import email  # Import the email module to parse email messages.\n",
    "from email.header import decode_header  # Import decode_header from email.header to decode email subject headers.\n",
    "\n",
    "\n",
    "''' Extract Text from PDF\n",
    "'''\n",
    "import pdfplumber  # Import pdfplumber to extract text from PDF files.\n",
    "\n",
    "\n",
    "''' Clean Extracted Text\n",
    "'''\n",
    "import nltk  # Import nltk (Natural Language Toolkit) for natural language processing tasks.\n",
    "from nltk.corpus import stopwords # Import stopwords from nltk.corpus to filter out common words in text processing. \n",
    "import re  # Import re (regular expressions) for pattern matching and text processing.\n",
    "\n",
    "\n",
    "''' Save Data in JSONL Format\n",
    "'''\n",
    "import jsonlines  # Import jsonlines to read and write JSON Lines (.jsonl) files.\n",
    "\n",
    "\n",
    "''' Load JSONL Data into Pandas\n",
    "'''\n",
    "import json  # For handling JSON files\n",
    "import pandas as pd  # For handling structured data in dataframe for data manipulation and analysis\n",
    "\n",
    "\n",
    "''' Web Scraping for LLM Training Data\n",
    "'''\n",
    "import requests  # For sending HTTP requests to web pages and APIs\n",
    "from bs4 import BeautifulSoup  # To Parses and extracts data from HTML and XML documents\n",
    "\n",
    "\n",
    "''' Fine-Tune a Pretrained LLM with Your Extracted Data\n",
    "'''\n",
    "# Import necessary modules from the Hugging Face Transformers library  \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments  \n",
    "# Import functions to load datasets from the Hugging Face Datasets library  \n",
    "from datasets import load_dataset  \n",
    "# Import DatasetDict to manage multiple dataset splits (train, validation, test)  \n",
    "from datasets import DatasetDict  \n",
    "# for optimizing distributed training with Hugging Face Transformers\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c90f10",
   "metadata": {},
   "source": [
    "#### 📌 Step 3 Option 1: Extract Emails from Gmail\n",
    "\n",
    "Run the following block of codes to fetch emails:\n",
    "\n",
    "NOTE: To Use Your Gmail for IMAP Access\n",
    "\n",
    "     1️⃣ Enable IMAP in Gmail\n",
    "        1. Go to Gmail Settings (⚙️ → \"See all settings\").\n",
    "        2. Navigate to the \"Forwarding and POP/IMAP\" tab.\n",
    "        3. Under IMAP access, select Enable IMAP and save changes.\n",
    "        \n",
    "     2️⃣ Generate an App Password (for security)\n",
    "        Since Google blocks less secure apps, you must use an App Password instead of your actual Gmail password.\n",
    "        1. Go to Google App Passwords.\n",
    "        2. Select Mail and choose the device (e.g., Windows, Mac).\n",
    "        3. Click Generate – Google will give you a 16-character password.\n",
    "        4. Use this password in your script instead of your Gmail password.\n",
    "     \n",
    "     Environment variables or a .env file is used for security reasons, to avoid making available the gmail details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ac685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gmail IMAP settings\n",
    "'''\n",
    "This setup is simply for security purposes to prevent \n",
    "hardcoding access codes directly on the worksheet.\n",
    "\n",
    "EDIT ACCESS DETAIL saved in .env file\n",
    "'''\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"gmail_details.env\")\n",
    "\n",
    "# Retrieve credentials\n",
    "EMAIL = os.getenv(\"EMAIL\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "IMAP_SERVER = os.getenv(\"IMAP_SERVER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ae2b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('OK', [b'101'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to Gmail\n",
    "mail = imaplib.IMAP4_SSL(IMAP_SERVER)\n",
    "mail.login(EMAIL, PASSWORD)\n",
    "mail.select(\"inbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51bf1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all emails\n",
    "status, messages = mail.search(None, \"ALL\")\n",
    "email_ids = messages[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1decadd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject': 'Whole has subscribed to you on YouTube!',\n",
       "  'body': \"Whole has subscribed to you on YouTube!\\r\\nChannels who subscribe to you will be notified when you upload new videos  \\r\\nor respond to others' videos (by favoriting, commenting, rating, etc). You  \\r\\ncan control which of your actions are publicly visible by going to your  \\r\\nSharing settings -  \\r\\nhttps://www.youtube.com/account_privacy?feature=em-subscription_create\\r\\nHelp Center - https://support.google.com/youtube\\r\\nEmail options -  \\r\\nhttps://www.youtube.com/account_notifications?feature=em-subscription_create\\r\\nUnsubscribe -  \\r\\nhttps://www.youtube.com/email_unsubscribe?uid=AcDEyFiAiOwI_2jbBZmxw3s9u9cz7E-unwCC1I-86ohMATfpb7kTxX47-OEC&action_unsubscribe=subscriber&timestamp=1738134675&feature=em-subscription_create\\r\\n(C) 2025 YouTube, LLC 901 Cherry Ave, San Bruno, CA 94066\\r\\n\"},\n",
       " {'subject': 'Andrea Haynes has subscribed to you on YouTube!',\n",
       "  'body': \"Andrea Haynes has subscribed to you on YouTube!\\r\\nChannels who subscribe to you will be notified when you upload new videos  \\r\\nor respond to others' videos (by favoriting, commenting, rating, etc). You  \\r\\ncan control which of your actions are publicly visible by going to your  \\r\\nSharing settings -  \\r\\nhttps://www.youtube.com/account_privacy?feature=em-subscription_create\\r\\nHelp Center - https://support.google.com/youtube\\r\\nEmail options -  \\r\\nhttps://www.youtube.com/account_notifications?feature=em-subscription_create\\r\\nUnsubscribe -  \\r\\nhttps://www.youtube.com/email_unsubscribe?uid=AcDEyFgZX-pd53kjieUTwwKn2CSDxaxUnMxxukz1xR48wfc3floChBlW_Dce&action_unsubscribe=subscriber&timestamp=1738217418&feature=em-subscription_create\\r\\n(C) 2025 YouTube, LLC 901 Cherry Ave, San Bruno, CA 94066\\r\\n\"},\n",
       " {'subject': 'Security alert',\n",
       "  'body': '[image: Google]\\r\\nA new sign-in on Windows\\r\\n\\r\\n\\r\\nrevelation.channel.tv@gmail.com\\r\\nWe noticed a new sign-in to your Google Account on a Windows device. If\\r\\nthis was you, you don’t need to do anything. If not, we’ll help you secure\\r\\nyour account.\\r\\nCheck activity\\r\\n<https://accounts.google.com/AccountChooser?Email=revelation.channel.tv@gmail.com&continue=https://myaccount.google.com/alert/nt/1740735782000?rfn%3D325%26rfnc%3D1%26eid%3D-8813670983778669745%26et%3D0>\\r\\nYou can also see security activity at\\r\\nhttps://myaccount.google.com/notifications\\r\\nYou received this email to let you know about important changes to your\\r\\nGoogle Account and services.\\r\\n© 2025 Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA\\r\\n'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract latest 5 emails\n",
    "emails = []\n",
    "for num in email_ids[-5:]:  # <-- Remove [-5:] to fetch all emails\n",
    "    status, msg_data = mail.fetch(num, \"(RFC822)\")\n",
    "    for response in msg_data:\n",
    "        if isinstance(response, tuple):\n",
    "            msg = email.message_from_bytes(response[1])\n",
    "            subject, encoding = decode_header(msg[\"Subject\"])[0]\n",
    "            if isinstance(subject, bytes):\n",
    "                subject = subject.decode(encoding or \"utf-8\")\n",
    "\n",
    "            # Get email body\n",
    "            body = \"\"\n",
    "            if msg.is_multipart():\n",
    "                for part in msg.walk():\n",
    "                    content_type = part.get_content_type()\n",
    "                    if content_type == \"text/plain\":\n",
    "                        body = part.get_payload(decode=True).decode(\"utf-8\")\n",
    "                        break\n",
    "            else:\n",
    "                body = msg.get_payload(decode=True).decode(\"utf-8\")\n",
    "\n",
    "            emails.append({\"subject\": subject, \"body\": body})\n",
    "\n",
    "mail.logout()\n",
    "\n",
    "# Display extracted emails\n",
    "emails[:3]  # Show first 3 emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e2319",
   "metadata": {},
   "source": [
    "#### 📌 Step 3 Option 2: Extract Text from PDFs\n",
    "Run this in a separate cell to extract text from PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32143070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba8f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert PDF file name into the function for text extraction\n",
    "pdf_text = extract_text_from_pdf(\"benny+hinn-+good+morning+holy+spirit.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701c2a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nd-warez.info/\n",
      "Good\n",
      "Morning,\n",
      "Holy\n",
      "Spirit\n",
      "Books by Benny Hinn from\n",
      "Thomas Nelson Publishers\n",
      "The Anointing\n",
      "The Biblical Road to Blessing\n",
      "Good Morning, Holy Spirit\n",
      "Welcome, Holy Spirit\n",
      "\n",
      "Copyright © 1990,1997 by Benny Hinn\n",
      "All rights reserved. Written permission must be secured from the publisher to use or reproduce\n",
      "any part of this book, except for brief quotations in critical reviews or articles.\n",
      "Published in Nashville, Tennessee, by Thomas Nelson, Inc.\n",
      "Scripture quotations are from THE NEW KING JAMES VERSION of the Bible. Copyright © 1979,\n",
      "1980, 1982, Thomas Nelson, Inc., Publishers.\n",
      "Library of Congress Cataloging-in-Publication Data\n",
      "Hinn, Benny.\n",
      "Good morning, Holy Spirit / Benny Hinn.\n",
      "p. cm.\n",
      "Includes bibliographical references.\n",
      "ISBN 0-7852-7176-7 (pbk.)\n",
      "1. Hinn, Benny. 2. Pentecostal churches—United States—Clergy-Biography. 3.\n",
      "Evangelists—United States—Biography. 4. Holy Spirit. I. Title.\n",
      "BX8762.Z8H5S 1997\n",
      "289.9'4'092-dc21\n",
      "[B] 97-5430\n",
      "CIP\n",
      "Printed in the United States of Amer\n"
     ]
    }
   ],
   "source": [
    "# Display extracted text\n",
    "print(pdf_text[:1000])  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9cfd61",
   "metadata": {},
   "source": [
    "#### 📌 Step 4: Clean Extracted Text\n",
    "Run this to clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c593086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f72f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "780dd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean email text\n",
    "for email_data in emails:\n",
    "    email_data[\"clean_body\"] = clean_text(email_data[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b1ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean PDF text\n",
    "pdf_text_clean = clean_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0bca032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'httpwwwndwarezinfo good morning holy spirit books benny hinn thomas nelson publishers anointing biblical road blessing good morning holy spirit welcome holy spirit copyright 19901997 benny hinn rights reserved written permission must secured publisher use reproduce part book except brief quotations critical reviews articles published nashville tennessee thomas nelson inc scripture quotations new king james version bible copyright 1979 1980 1982 thomas nelson inc publishers library congress cataloginginpublication data hinn benny good morning holy spirit benny hinn p cm includes bibliographical references isbn 0785271767 pbk 1 hinn benny 2 pentecostal churchesunited statesclergybiography 3 evangelistsunited statesbiography 4 holy spirit title bx8762z8h5s 1997 28994092dc21 b 975430 cip printed united states america 48 01 00 99 98 dedication person holy spirit reason daughters jessica natasha lord tarry carry message generation contents acknowledgmentsviii 1 really know you11 2 jaffa ends'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display cleaned data\n",
    "pdf_text_clean[:1000]  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759c20a",
   "metadata": {},
   "source": [
    "#### 📌 Step 5: Web Scraping for LLM Training Data\n",
    "Run this in a separate cell to extract text from PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb78987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#🔸 Scrape Web Pages for Text Data\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrapes text content from a webpage\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text_data = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66a1ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scrape Wikipedia\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "data = scrape_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0665c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Web scraping complete. Data saved!\n"
     ]
    }
   ],
   "source": [
    "# Save to file for LLM fine-tuning\n",
    "with open(\"scraped_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "print(\"✅ Web scraping complete. Data saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29357f",
   "metadata": {},
   "source": [
    "#### 📌 Step 6: Save Data in JSONL Format\n",
    "Run this to store the structured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec19a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email & PDF data saved successfully in data.jsonl!\n"
     ]
    }
   ],
   "source": [
    "# Save extracted email data and PDF text\n",
    "with jsonlines.open(\"data.jsonl\", \"w\") as file:\n",
    "    # Save email data\n",
    "    for email_data in emails:\n",
    "        file.write({\"source\": \"email\", \"text\": email_data[\"clean_body\"]})\n",
    "\n",
    "    # Save PDF text\n",
    "    file.write({\"source\": \"pdf\", \"text\": pdf_text_clean})\n",
    "\n",
    "    \n",
    "print(\"Email & PDF data saved successfully in data.jsonl!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "200b8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Web data converted and saved in data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Convert scraped_data.txt to JSONL (Append to data.jsonl)\n",
    "\n",
    "def convert_text_file_to_jsonl(input_file, output_file, source_type=\"web\"):\n",
    "    \"\"\"\n",
    "    Reads a text file line by line, converts it to JSONL format, and appends it to the existing dataset.\n",
    "    \n",
    "    :param input_file: The input text file containing extracted web data.\n",
    "    :param output_file: The output JSONL file where data will be stored.\n",
    "    :param source_type: The source label (default: \"web\").\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = f.readlines()  # Read all lines from the text file\n",
    "\n",
    "    with jsonlines.open(output_file, \"a\") as f:  # Open in append mode\n",
    "        for text in data_list:\n",
    "            f.write({\"source\": source_type, \"text\": text.strip()})  # Write each line as JSON\n",
    "\n",
    "# Convert scraped text and append to JSONL file\n",
    "convert_text_file_to_jsonl(\"scraped_data.txt\", \"data.jsonl\", source_type=\"web\")\n",
    "\n",
    "print(\"✅ Web data converted and saved in data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e11a99",
   "metadata": {},
   "source": [
    "#### 📌 Extra Step: Load JSONL Data into Pandas\n",
    "Run this to:\n",
    "\n",
    "    1. Reads the data.jsonl file line by line.\n",
    "    2. Converts it into a structured Pandas DataFrame.\n",
    "    3. Displays the first few rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f841a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSONL data into a list\n",
    "data = []\n",
    "with jsonlines.open(\"data.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(line)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b10ebe82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>email</td>\n",
       "      <td>whole subscribed youtube channels subscribe no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>email</td>\n",
       "      <td>andrea haynes subscribed youtube channels subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>email</td>\n",
       "      <td>image google new signin windows revelationchan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>email</td>\n",
       "      <td>image google account recovered successfully re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>email</td>\n",
       "      <td>image google app password created sign account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pdf</td>\n",
       "      <td>httpwwwndwarezinfo good morning holy spirit bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               text\n",
       "0  email  whole subscribed youtube channels subscribe no...\n",
       "1  email  andrea haynes subscribed youtube channels subs...\n",
       "2  email  image google new signin windows revelationchan...\n",
       "3  email  image google account recovered successfully re...\n",
       "4  email  image google app password created sign account...\n",
       "5    pdf  httpwwwndwarezinfo good morning holy spirit bo..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc090294",
   "metadata": {},
   "source": [
    "### ✅ Fine-Tuning an Open-Source LLM (Example: LLaMA 2 or GPT-2) Using Extracted Data\n",
    "Once you’ve gathered data from emails, PDF, and web scraping, you need to format it for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d352d5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c4ad58596d46a8ad5715c4be56837c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset from JSONL file\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"data.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "575f0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Pretrained Model (LLaMA-2 or GPT-2)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8924a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolves Tokenizer Padding Issue by Set the padding token explicitly before tokenization\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f8714ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfed17ac240c432b92e14f4d8371ce6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c6e14ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a52a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split ratio (e.g., 90% train, 10% validation)\n",
    "split_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Rename test set to validation\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": split_datasets[\"train\"],\n",
    "    \"validation\": split_datasets[\"test\"]  # Rename test set to validation\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bda8e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8097f65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a593cad729314adbab2988d7cc4d60e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb1a3b4bff24472afbae4258dd10a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check If Model Requires labels During Training\n",
    "def add_labels(batch):\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()  # Copy input_ids as labels\n",
    "    return batch\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(add_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6346b733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d841807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 38:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.170684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.150106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27, training_loss=1.4366320857295283, metrics={'train_runtime': 2370.2296, 'train_samples_per_second': 0.046, 'train_steps_per_second': 0.011, 'total_flos': 56439078912000.0, 'train_loss': 1.4366320857295283, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f9bd458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_llm\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_llm\")\n",
    "\n",
    "print(\"✅ Fine-tuning complete! Model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215691c",
   "metadata": {},
   "source": [
    "### Load the Fine-Tuned Model to Test our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8840b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"fine_tuned_llm\"  # Update with your saved model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8428fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, max_length=max_length, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ec90877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Enter a prompt: What is Data Science?\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample prompt\n",
    "\n",
    "# Request user input for the prompt (ASK: What is data science?) ENSURE it ends with a ? mark\n",
    "prompt = input(\"🔹 Enter a prompt: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "136eb931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Model Output:\n",
      "What is Data Science?\n",
      "\n",
      "Data science is a field of study that uses data to understand and understand the world around us. Data science is a field of study that uses data to understand and understand the world around us.\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on user input\n",
    "generated_text = generate_text(prompt)\n",
    "\n",
    "# Display the model's response\n",
    "print(\"\\n📝 Model Output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fefe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
